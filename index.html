<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Drowsiness Detector</title>
  <style>
    body {
      margin: 0;
      background: #121212;
      color: #fff;
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
    }

    h1 {
      margin-bottom: 10px;
    }

    .video-container {
      position: relative;
      width: 512px;
      height: 512px;
      border: 2px solid #444;
      border-radius: 12px;
      overflow: hidden;
      margin-bottom: 20px;
    }

    #video, #canvas {
      position: absolute;
      width: 512px;
      height: 512px;
      top: 0;
      left: 0;
      object-fit: cover;
    }

    #statusPanel {
      width: 512px;
      background: #1e1e1e;
      padding: 15px;
      border-radius: 8px;
      box-shadow: 0 0 10px #000;
      font-size: 16px;
    }

    #alertBox {
      color: #ff4c4c;
      font-weight: bold;
      font-size: 18px;
      margin-bottom: 10px;
    }

    .label {
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>Drowsiness Detector</h1>

  <div class="video-container">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="canvas"></canvas>
  </div>

  <div id="statusPanel">
    <div id="alertBox"></div>
    <div><span class="label">Status:</span> <span id="status">Initializing...</span></div>
    <div><span class="label">EAR:</span> <span id="earValue">-</span></div>
  </div>

  <!-- TensorFlow + MediaPipe -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>

  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");

    const alertBox = document.getElementById("alertBox");
    const statusText = document.getElementById("status");
    const earDisplay = document.getElementById("earValue");

    const EAR_THRESHOLD = 0.25;
    const LEFT_EYE = [362, 385, 387, 263, 373, 380];
    const RIGHT_EYE = [33, 160, 158, 133, 153, 144];
    let earHistory = [];
    let model;

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => resolve();
      });
    }

    function getEAR(eye) {
      const v1 = Math.hypot(eye[1][0] - eye[5][0], eye[1][1] - eye[5][1]);
      const v2 = Math.hypot(eye[2][0] - eye[4][0], eye[2][1] - eye[4][1]);
      const h = Math.hypot(eye[0][0] - eye[3][0], eye[0][1] - eye[3][1]);
      return (v1 + v2) / (2.0 * h);
    }

    function getEyeCoords(landmarks, indices, w, h) {
      return indices.map(i => [landmarks[i].x * w, landmarks[i].y * h]);
    }

    async function detect() {
      const faces = await model.estimateFaces({ input: video });
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      if (faces.length > 0) {
        const kp = faces[0].keypoints;

        const leftEye = getEyeCoords(kp, LEFT_EYE, canvas.width, canvas.height);
        const rightEye = getEyeCoords(kp, RIGHT_EYE, canvas.width, canvas.height);

        // Draw eyes
        [...leftEye, ...rightEye].forEach(([x, y]) => {
          ctx.beginPath();
          ctx.arc(x, y, 2, 0, 2 * Math.PI);
          ctx.fillStyle = "lime";
          ctx.fill();
        });

        const earL = getEAR(leftEye);
        const earR = getEAR(rightEye);
        const ear = ((earL + earR) / 2).toFixed(3);
        earDisplay.textContent = ear;

        earHistory.push(parseFloat(ear));
        if (earHistory.length > 10) earHistory.shift();

        const avgEAR = earHistory.reduce((a, b) => a + b, 0) / earHistory.length;

        if (avgEAR < EAR_THRESHOLD) {
          alertBox.textContent = "😴 DROWSINESS ALERT!";
        } else {
          alertBox.textContent = "";
        }

        statusText.textContent = "Face Detected ✅";
      } else {
        statusText.textContent = "No Face Detected ❌";
        earDisplay.textContent = "-";
        alertBox.textContent = "";
      }

      requestAnimationFrame(detect);
    }

    async function main() {
      statusText.textContent = "Requesting Camera...";
      await setupCamera();
      video.play();

      await new Promise(resolve => {
        if (video.readyState >= 2) resolve();
        else video.onloadeddata = resolve;
      });

      canvas.width = 512;
      canvas.height = 512;

      statusText.textContent = "Loading Model...";
      model = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
      );

      statusText.textContent = "Running Detection...";
      detect();
    }

    main();
  </script>
</body>
</html>
